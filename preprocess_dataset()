import os
import cv2
import nltk
from nltk.tokenize import word_tokenize
from tqdm import tqdm

def preprocess_dataset(data_dir, output_dir, dataset='CUB-200'):
    # Create the output directory if it does not exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Load the image file names and text descriptions from the dataset
    if dataset == 'CUB-200':
        with open(os.path.join(data_dir, 'files.txt'), 'r') as f:
            image_files = [line.strip() for line in f]
        with open(os.path.join(data_dir, 'text_c10.txt'), 'r') as f:
            text_descriptions = [line.strip() for line in f]
    elif dataset == 'Oxford-102':
        with open(os.path.join(data_dir, 'train.txt'), 'r') as f:
            image_files = [line.strip().split()[0] for line in f]
        with open(os.path.join(data_dir, 'train.txt'), 'r') as f:
            text_descriptions = [line.strip().split()[1:] for line in f]
        text_descriptions = [' '.join(desc) for desc in text_descriptions]

    # Tokenize the text descriptions
    tokenized_descriptions = [word_tokenize(d.lower()) for d in text_descriptions]

    # Preprocess each image and text description and save the results
    for i, (image_file, description) in tqdm(enumerate(zip(image_files, tokenized_descriptions))):
        # Load the image and convert it to grayscale
        image = cv2.imread(os.path.join(data_dir, 'images', image_file))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Resize the image to 256x256
        image = cv2.resize(image, (256, 256))

        # Save the preprocessed image and text description
        cv2.imwrite(os.path.join(output_dir, f'{i}.jpg'), image)
        with open(os.path.join(output_dir, f'{i}.txt'), 'w') as f:
            f.write(' '.join(description))
